"""
é‡‘èæŠ¥å‘Šæ™ºèƒ½è§£æå™¨ - Finance Agent Listener æ’ä»¶
ç›‘å¬æŠ¥å‘Šå¯¼å…¥äº‹ä»¶,è‡ªåŠ¨æå–å…³é”®ä¿¡æ¯å¹¶å­˜å‚¨åˆ°æ•°æ®åº“

åŠŸèƒ½:
- è‡ªåŠ¨åˆ†ææ–°å¯¼å…¥çš„é‡‘èæŠ¥å‘Š
- æå–ç»“æ„åŒ–ä¿¡æ¯(æŠ•èµ„å»ºè®®ã€é£é™©è¯„ä¼°ã€å…³é”®æ•°æ®ç­‰)
- å­˜å‚¨åˆ° SQLite æ•°æ®åº“(reports è¡¨)
- æ”¯æŒå…¨æ–‡æœç´¢(FTS5)
- é€šçŸ¥ç”¨æˆ·åˆ†æå®Œæˆ
"""

import json
import os
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from pathlib import Path

# Claude Agent SDK imports
try:
    from claude_agent_sdk import (
        AssistantMessage,
        TextBlock,
        query
    )
except ImportError:
    print("è¯·å…ˆå®‰è£…ä¾èµ–: pip install claude-agent-sdk")
    raise

# é¡¹ç›®å†…éƒ¨å¯¼å…¥
import sys
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from database.database_manager import DatabaseManager


# ============================================================================
# Listener é…ç½® (å¿…éœ€å¯¼å‡º)
# ============================================================================

config = {
    "id": "report_analyzer",
    "name": "é‡‘èæŠ¥å‘Šæ™ºèƒ½åˆ†æå™¨",
    "description": "è‡ªåŠ¨åˆ†ææ–°å¯¼å…¥çš„é‡‘èæŠ¥å‘Š,æå–æŠ•èµ„å»ºè®®ã€é£é™©è¯„ä¼°ã€å…³é”®æ•°æ®ç­‰ç»“æ„åŒ–ä¿¡æ¯",
    "enabled": True,
    "event": "report_added"  # ç›‘å¬æŠ¥å‘Šæ·»åŠ äº‹ä»¶ï¼ˆç»Ÿä¸€äº‹ä»¶åï¼‰
}


def _extract_text_summary(report_text: str) -> Dict:
    """æå–æ–‡æœ¬æ‘˜è¦ã€æ ¸å¿ƒè§‚ç‚¹å’Œåˆ†ææ¡†æ¶"""
    import re
    
    # å…ˆç»Ÿä¸€å¤„ç†æ–‡æœ¬ï¼šå»é™¤æ‰€æœ‰ç©ºæ ¼å’Œæ¢è¡Œï¼Œç„¶åæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å¥
    # è¿™æ ·å¯ä»¥å¤„ç†OCRæ–‡æœ¬ä¸­å­—ä¸å­—ä¹‹é—´æœ‰ç©ºæ ¼çš„æƒ…å†µ
    cleaned_text = re.sub(r'\s+', '', report_text.strip())
    
    # æŒ‰å¥å­åˆ†å‰²ï¼ˆå¤„ç†ä¸­æ–‡æ ‡ç‚¹ï¼‰
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ;ï¼›]', cleaned_text)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
    
    # æå–æ ¸å¿ƒè§‚ç‚¹ - å¯»æ‰¾åŒ…å«åˆ¤æ–­æ€§è¯æ±‡çš„å®Œæ•´å¥å­
    core_views = []
    judgment_keywords = ['åº”å½“', 'éœ€è¦', 'å»ºè®®', 'å¯ä»¥', 'ä¸ä¼š', 'ä¼š', 'å°†', 'å¿…ç„¶', 'é¢„æœŸ', 'åˆ¤æ–­', 'è®¤ä¸º', 'çœ‹å¥½', 'çœ‹ç©º', 'åº”è¯¥', 'èƒ½å¤Ÿ', 'å¯èƒ½']
    
    for sentence in sentences:
        if len(sentence) > 20 and any(keyword in sentence for keyword in judgment_keywords):
            # é¿å…è¿‡é•¿çš„å¥å­ï¼Œä½†ä¿ç•™è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡
            if len(sentence) <= 150:
                core_views.append(sentence)
            else:
                # å¯¹äºè¶…é•¿å¥å­ï¼Œå°è¯•ä»å…³é”®è¯é™„è¿‘æå–
                for keyword in judgment_keywords:
                    if keyword in sentence:
                        # æ‰¾åˆ°å…³é”®è¯ä½ç½®
                        idx = sentence.index(keyword)
                        # æå–å…³é”®è¯å‰åå„75ä¸ªå­—ç¬¦
                        start = max(0, idx - 50)
                        end = min(len(sentence), idx + 100)
                        fragment = sentence[start:end]
                        if len(fragment) > 20:
                            core_views.append(fragment)
                            break
    
    # æå–å…³é”®æ•°æ® - å¯»æ‰¾åŒ…å«æ•°å­—çš„å®Œæ•´å¥å­
    key_facts = []
    # å¢å¼ºæ•°å­—åŒ¹é…æ¨¡å¼
    number_pattern = r'\d+\.?\d*[%äº¿ä¸‡å…ƒç¾å…ƒç‚¹ç›å¸å¨ä¸ªå®¶ä»½æœˆå¹´æ—¥]|\d+[-~è‡³]\d+|\d{4}å¹´|\d+æœˆ|\d+æ—¥'
    
    for sentence in sentences:
        if re.search(number_pattern, sentence) and len(sentence) > 15:
            # åŒæ ·å¤„ç†è¿‡é•¿å¥å­
            if len(sentence) <= 150:
                key_facts.append(sentence)
            else:
                # æå–åŒ…å«æ•°å­—çš„å…³é”®éƒ¨åˆ†
                matches = list(re.finditer(number_pattern, sentence))
                if matches:
                    # å–ç¬¬ä¸€ä¸ªæ•°å­—é™„è¿‘çš„å†…å®¹
                    match = matches[0]
                    idx = match.start()
                    start = max(0, idx - 40)
                    end = min(len(sentence), idx + 110)
                    fragment = sentence[start:end]
                    if len(fragment) > 15:
                        key_facts.append(fragment)
    
    # æå–åˆ†ææ¡†æ¶ - å¯»æ‰¾æ¡†æ¶æ€§æè¿°çš„å®Œæ•´å¥å­
    analysis_framework = []
    framework_keywords = ['å› ç´ ', 'æ¡†æ¶', 'æ–¹æ³•', 'é€»è¾‘', 'æ¨¡å‹', 'ç»´åº¦', 'ç‰¹å¾', 'åŸåˆ™', 'ç­–ç•¥', 'æœºåˆ¶', 'é©±åŠ¨', 'è§’åº¦', 'æ–¹é¢']
    
    for sentence in sentences:
        if len(sentence) > 20 and any(keyword in sentence for keyword in framework_keywords):
            if len(sentence) <= 150:
                analysis_framework.append(sentence)
            else:
                # å¯¹äºè¶…é•¿å¥å­ï¼Œæå–å…³é”®è¯é™„è¿‘å†…å®¹
                for keyword in framework_keywords:
                    if keyword in sentence:
                        idx = sentence.index(keyword)
                        start = max(0, idx - 50)
                        end = min(len(sentence), idx + 100)
                        fragment = sentence[start:end]
                        if len(fragment) > 20:
                            analysis_framework.append(fragment)
                            break
    
    # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
    def deduplicate(items):
        seen = set()
        result = []
        for item in items:
            if item not in seen and len(item) > 10:
                seen.add(item)
                result.append(item)
        return result
    
    core_views = deduplicate(core_views)
    key_facts = deduplicate(key_facts)
    analysis_framework = deduplicate(analysis_framework)
    
    return {
        "core_views": core_views[:15],  # æœ€å¤š15æ¡æ ¸å¿ƒè§‚ç‚¹
        "key_facts": key_facts[:20],    # æœ€å¤š20æ¡å…³é”®æ•°æ®
        "analysis_framework": analysis_framework[:12]  # æœ€å¤š12æ¡åˆ†ææ¡†æ¶
    }


def _build_analysis_prompt(report_text: str, depth: str) -> str:
    """æ ¹æ®åˆ†ææ·±åº¦æ„å»ºæç¤ºè¯"""
    
    base_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆ,è¯·åˆ†æä»¥ä¸‹æŠ¥å‘Š:

{report_text}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚"""

    if depth == "quick":
        base_prompt += """
åªéœ€è¦æä¾›ï¼š
- æŠ¥å‘Šç±»å‹å’Œæ—¥æœŸ
- ä¸€å¥è¯æ€»ç»“
- 3ä¸ªæ ¸å¿ƒè§‚ç‚¹
- ç®€å•çš„æŠ•èµ„å»ºè®®
"""
    elif depth == "deep":
        base_prompt += """
éœ€è¦æ·±åº¦åˆ†æï¼š
- è¯¦ç»†çš„å› æœå…³ç³»åˆ†æ
- å¤šç»´åº¦çš„é£é™©è¯„ä¼°
- é‡åŒ–çš„è¯„åˆ†æ¨¡å‹
- å…·ä½“çš„æ“ä½œç­–ç•¥
- å…³è”èµ„äº§åˆ†æ
"""
    
    base_prompt += """
JSONæ ¼å¼è¦æ±‚ï¼ˆè¯·å°½å¯èƒ½è¯¦ç»†æå–æ‰€æœ‰ä¿¡æ¯ï¼‰ï¼š
{
  "report_info": {
    "type": "æŠ¥å‘Šç±»å‹",
    "category": "å…·ä½“åˆ†ç±»",
    "date": "æ—¥æœŸ",
    "title": "æ ‡é¢˜",
    "sources": ["ä¿¡æ¯æ¥æº1", "ä¿¡æ¯æ¥æº2"]  // æ–°å¢ï¼šæ ‡æ³¨æŠ¥å‘Šå¼•ç”¨çš„æ•°æ®æ¥æº
  },
  "summary": {
    "one_sentence": "ä¸€å¥è¯æ€»ç»“",
    "sentiment": "bullish/bearish/neutral",
    "key_drivers": ["é©±åŠ¨å› ç´ 1", "é©±åŠ¨å› ç´ 2"]  // æ–°å¢ï¼šæ ¸å¿ƒé©±åŠ¨å› ç´ 
  },
  "key_data": {
    "å…³é”®æŒ‡æ ‡": "æ•°å€¼"  // å°½å¯èƒ½æå–æ‰€æœ‰æ•°å­—ã€ç™¾åˆ†æ¯”ã€é‡‘é¢ã€æ—¶é—´èŠ‚ç‚¹ç­‰
  },
  "historical_context": {  // æ–°å¢ï¼šå†å²å¯¹æ¯”æ•°æ®
    "å¯¹æ¯”é¡¹ç›®": "å†å²æ•°æ®å¯¹æ¯”",
    "è¶‹åŠ¿å˜åŒ–": "æè¿°è¶‹åŠ¿è½¬æŠ˜ç‚¹"
  },
  "main_points": ["è§‚ç‚¹1", "è§‚ç‚¹2", "è§‚ç‚¹3"],
  "investment_targets": {  // æ–°å¢ï¼šå…·ä½“æŠ•èµ„æ ‡çš„åˆ†æ
    "recommended": [  // æ¨èæ ‡çš„
      {
        "name": "å…¬å¸/ETFåç§°",
        "type": "ä¸ªè‚¡/ETF/æ¿å—",
        "reason": "æ¨èç†ç”±",
        "key_metrics": "å…³é”®è´¢åŠ¡/ä¸šç»©æ•°æ®",
        "price_action": "è‚¡ä»·è¡¨ç°æ•°æ®",
        "market_share": "å¸‚åœºä»½é¢ç­‰ä¿¡æ¯"
      }
    ],
    "cautious": [  // éœ€è°¨æ…çš„æ ‡çš„
      {
        "name": "æ ‡çš„åç§°",
        "reason": "è°¨æ…ç†ç”±",
        "risk_factors": "é£é™©å› ç´ "
      }
    ]
  },
  "investment_advice": {
    "action": "buy/sell/hold/watch",
    "target_allocation": "é…ç½®æ¯”ä¾‹å»ºè®®",
    "timing": "æ“ä½œæ—¶æœºå»ºè®®",  // æ–°å¢ï¼šæ—¶æœºå»ºè®®
    "holding_period": "å»ºè®®æŒæœ‰å‘¨æœŸ",  // æ–°å¢ï¼šæŒæœ‰æœŸå»ºè®®
    "confidence_level": "high/medium/low"
  },
  "risk_warnings": [  // è¯¦ç»†é£é™©æç¤º
    {
      "risk_type": "é£é™©ç±»å‹",
      "description": "å…·ä½“é£é™©æè¿°",
      "impact_level": "high/medium/low",
      "affected_targets": ["å—å½±å“æ ‡çš„"]
    }
  ],
  "timeline_events": [  // æ–°å¢ï¼šå…³é”®æ—¶é—´èŠ‚ç‚¹
    {
      "date": "æ—¥æœŸ",
      "event": "äº‹ä»¶æè¿°",
      "impact": "å½±å“åˆ†æ"
    }
  ],
  "industry_structure": {  // æ–°å¢ï¼šäº§ä¸šç»“æ„åˆ†æ
    "supply_chain": "äº§ä¸šé“¾åˆ†æ",
    "competitive_landscape": "ç«äº‰æ ¼å±€",
    "barriers_to_entry": "è¡Œä¸šå£å’"
  },
  "quantitative_metrics": {  // æ–°å¢:é‡åŒ–æŒ‡æ ‡
    "investment_scale": "æŠ•èµ„è§„æ¨¡æ•°æ®",
    "growth_rates": "å¢é•¿ç‡æ•°æ®",
    "market_size": "å¸‚åœºè§„æ¨¡æ•°æ®",
    "capacity_data": "äº§èƒ½/è£…æœºç­‰æ•°æ®"
  },
  "text_summary": {  // æ–°å¢:æ–‡æœ¬æ‘˜è¦(AIæ™ºèƒ½æå–)
    "core_views": [  // æ ¸å¿ƒè§‚ç‚¹ï¼šé«˜åº¦æ¦‚æ‹¬çš„æŠ•èµ„åˆ¤æ–­å’Œç­–ç•¥å»ºè®®ï¼ˆ5-10æ¡ï¼‰
      "æ ¸å¿ƒè§‚ç‚¹1ï¼šç®€æ´é™ˆè¿°å¸‚åœºåˆ¤æ–­æˆ–æŠ•èµ„ç­–ç•¥",
      "æ ¸å¿ƒè§‚ç‚¹2ï¼š..."
    ],
    "key_facts": [  // å…³é”®æ•°æ®äº‹å®ï¼šæ”¯æ’‘è§‚ç‚¹çš„å…·ä½“æ•°å­—ã€æ¯”ä¾‹ã€é‡‘é¢ç­‰ï¼ˆ10-15æ¡ï¼‰
      "å…·ä½“æ•°æ®1ï¼šåŒ…å«æ•°å­—çš„å…³é”®äº‹å®",
      "å…·ä½“æ•°æ®2ï¼š..."
    ],
    "analysis_framework": [  // åˆ†ææ¡†æ¶ï¼šä½œè€…ä½¿ç”¨çš„åˆ†ææ–¹æ³•ã€é€»è¾‘ä½“ç³»ï¼ˆ3-8æ¡ï¼‰
      "åˆ†ææ¡†æ¶1ï¼šå¦‚'å››å¤§å› ç´ é©±åŠ¨æ¨¡å‹'",
      "åˆ†ææ¡†æ¶2ï¼š..."
    ]
  },
  "key_metrics": {
    "importance_score": 8,
    "urgency_score": 7,
    "reliability_score": 9
  }
}

ï¿½ï¿½åˆ«æç¤ºï¼š
1. è¯·æå–åŸæ–‡ä¸­æ‰€æœ‰å…·ä½“çš„æ•°å­—ã€ç™¾åˆ†æ¯”ã€é‡‘é¢ã€æ—¶é—´ç­‰æ•°æ®
2. å¯¹äºæåˆ°çš„æ‰€æœ‰å…¬å¸åç§°ï¼Œå°½å¯èƒ½æå–å…¶ä¸šç»©ã€è‚¡ä»·ã€å¸‚åœºä»½é¢ç­‰ä¿¡æ¯
3. æ ‡æ³¨å†å²å¯¹æ¯”æ•°æ®ï¼ˆå¦‚"åå¹´å‰vsç°åœ¨"ã€"å»å¹´vsä»Šå¹´"ç­‰ï¼‰
4. è®°å½•æ‰€æœ‰æ—¶é—´èŠ‚ç‚¹å’Œå…³é”®äº‹ä»¶
5. åŒºåˆ†çŸ­æœŸã€ä¸­æœŸã€é•¿æœŸçš„æŠ•èµ„é€»è¾‘

**text_summary å­—æ®µå¡«å†™è¦æ±‚**ï¼ˆé‡è¦ï¼‰ï¼š
- core_views: æç‚¼æŠ¥å‘Šçš„æ ¸å¿ƒæŠ•èµ„è§‚ç‚¹å’Œåˆ¤æ–­ï¼Œæ¯æ¡åº”æ˜¯å®Œæ•´ã€ç‹¬ç«‹ã€æ˜“ç†è§£çš„é™ˆè¿°ï¼ˆ20-80å­—ï¼‰
  ç¤ºä¾‹ï¼š"Aè‚¡ä¸å¤§æ¦‚ç‡æ­¢æ­¥äº4000ç‚¹ï¼Œå½“å‰å›´ç»•4000ç‚¹çš„æ‹‰é”¯å±å¹´æœ«ä¸Šæ¶¨åçš„æ­£å¸¸è°ƒæ•´ä¸æµåŠ¨æ€§é˜¶æ®µæ€§åç´§"
- key_facts: æå–æ”¯æ’‘è§‚ç‚¹çš„å…·ä½“æ•°æ®å’Œäº‹å®ï¼Œä¿æŒåŸæ–‡å‡†ç¡®æ€§ï¼ˆ15-60å­—ï¼‰
  ç¤ºä¾‹ï¼š"ä¸Šè¯æŒ‡æ•°æ¶¨å¹…çº¦45%"ã€"Aè‚¡ä¸‰å­£æŠ¥è¥æ”¶å¢é•¿5%ä»¥ä¸Šã€å‡€åˆ©æ¶¦å¢é•¿11%ä»¥ä¸Š"
- analysis_framework: æ€»ç»“ä½œè€…ä½¿ç”¨çš„åˆ†ææ–¹æ³•å’Œé€»è¾‘æ¡†æ¶ï¼ˆ20-80å­—ï¼‰
  ç¤ºä¾‹ï¼š"å››å¤§å› å­ç»¼åˆè¯„ä¼°æ³•ï¼šæ”¿ç­–é¢ã€å®è§‚é¢ã€èµ„é‡‘é¢ã€ä¸Šå¸‚å…¬å¸åŸºæœ¬é¢"
"""
    return base_prompt


def _parse_json_response(text: str) -> Dict:
    """è§£æClaudeçš„å“åº”"""
    import re
    
    # å°è¯•æå–JSON
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group())
        except:
            pass
    
    return {"error": "æ— æ³•è§£æå“åº”", "raw_text": text}


# ============================================================================
# Listener å¤„ç†å‡½æ•° (å¿…éœ€å¯¼å‡º)
# ============================================================================

async def handler(event_data: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    æŠ¥å‘Šåˆ†æ Listener å¤„ç†å‡½æ•°
    
    Args:
        event_data: {
            'file_path': '/path/to/report.txt',
            'filename': 'report.txt',
            'content': 'æŠ¥å‘ŠåŸæ–‡...',
            'report_id': 'optional_custom_id',
            'skip_analysis': False  # æ–°å¢ï¼šå¦‚æœä¸º Trueï¼Œè·³è¿‡åˆ†æï¼ˆé¿å…é‡å¤ï¼‰
        }
        context: ListenerContext {
            notify(message, options),
            callAgent(options),
            uiState.get/set(stateId, data)
        }
    
    Returns:
        {
            'executed': True/False,
            'reason': 'æ‰§è¡ŒåŸå› ',
            'actions': ['åˆ†æå®Œæˆ', 'å·²å­˜å‚¨'],
            'report_id': 'ç”Ÿæˆçš„æŠ¥å‘ŠID'
        }
    """
    # æ£€æŸ¥æ˜¯å¦è·³è¿‡åˆ†æï¼ˆé¿å… report_service è°ƒç”¨åé‡å¤åˆ†æï¼‰
    if event_data.get('skip_analysis'):
        return {
            'executed': False,
            'reason': 'å·²ç”± API åˆ†æï¼Œè·³è¿‡ Listener å¤„ç†'
        }
    
    # è·å–äº‹ä»¶æ•°æ®
    file_path = event_data.get('file_path')
    filename = event_data.get('filename', 'unknown')
    content = event_data.get('content')
    custom_report_id = event_data.get('report_id')
    
    # éªŒè¯è¾“å…¥
    if not content or len(content.strip()) < 50:
        return {
            'executed': False,
            'reason': 'æŠ¥å‘Šå†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­'
        }
    
    try:
        # æ­¥éª¤ 1: ä½¿ç”¨ AI åˆ†ææŠ¥å‘Š
        print(f"[æŠ¥å‘Šåˆ†æå™¨] æ­£åœ¨åˆ†æ: {filename}")
        analysis_result = await _analyze_report_with_ai(content, depth="standard")
        
        if "error" in analysis_result:
            await context.notify(
                f"âŒ æŠ¥å‘Šåˆ†æå¤±è´¥: {filename}\nåŸå› : {analysis_result['error']}",
                {"priority": "normal"}
            )
            return {
                'executed': False,
                'reason': f"AIåˆ†æå¤±è´¥: {analysis_result['error']}"
            }
        
        # æ­¥éª¤ 2: å‡†å¤‡æ•°æ®åº“å­˜å‚¨æ ¼å¼
        report_data = _transform_to_db_format(analysis_result, filename, file_path, custom_report_id)
        
        # æ­¥éª¤ 3: å­˜å‚¨åˆ°æ•°æ®åº“
        db = DatabaseManager()
        report_id = await db.upsert_report(report_data)
        
        # æ­¥éª¤ 4: å‘é€é€šçŸ¥
        report_info = analysis_result.get('report_info', {})
        summary = analysis_result.get('summary', {})
        
        notification_msg = f"""âœ… æŠ¥å‘Šåˆ†æå®Œæˆ

ğŸ“‹ {report_info.get('title', filename)}
åˆ†ç±»: {report_info.get('category', 'N/A')}
æƒ…ç»ª: {summary.get('sentiment', 'N/A')}
é‡è¦æ€§: {analysis_result.get('key_metrics', {}).get('importance_score', 'N/A')}/10

ğŸ’¡ {summary.get('one_sentence', '')}
"""
        
        await context.notify(notification_msg, {"priority": "normal"})
        
        # æ­¥éª¤ 5: æ›´æ–° UI çŠ¶æ€ (å¯é€‰)
        # å°†æœ€æ–°æŠ¥å‘Šæ·»åŠ åˆ°ä»ªè¡¨æ¿
        try:
            dashboard_state = await context.uiState.get("financial_dashboard")
            if dashboard_state and isinstance(dashboard_state, dict):
                recent_reports = dashboard_state.get('recent_reports', [])
                recent_reports.insert(0, {
                    'report_id': report_data['report_id'],
                    'title': report_data.get('title', filename),
                    'category': report_data.get('category'),
                    'date': report_data.get('date_published'),
                    'importance_score': report_data.get('importance_score')
                })
                dashboard_state['recent_reports'] = recent_reports[:10]  # ä¿ç•™æœ€è¿‘10æ¡
                await context.uiState.set("financial_dashboard", dashboard_state)
        except Exception as e:
            print(f"[è­¦å‘Š] æ›´æ–°ä»ªè¡¨æ¿çŠ¶æ€å¤±è´¥: {e}")
        
        # æ­¥éª¤ 6: æ‰§è¡Œå…³è”æ€§åˆ†æ
        print(f"[DEBUG] [æŠ¥å‘Šåˆ†æå™¨] [handler] æ­¥éª¤6: å¼€å§‹æ‰§è¡Œå…³è”æ€§åˆ†æ, report_id={report_data['report_id']}")
        try:
            from database.relationship_analyzer import ReportRelationshipAnalyzer
            print(f"[DEBUG] [æŠ¥å‘Šåˆ†æå™¨] [handler] åˆå§‹åŒ–å…³è”åˆ†æå™¨")
            relationship_analyzer = ReportRelationshipAnalyzer(db)
            # æ‰§è¡Œå…³è”åˆ†æï¼ˆæŠ¥å‘Šå·²ç»å­˜å‚¨åˆ°ChromaDBä¸­ï¼‰
            print(f"[DEBUG] [æŠ¥å‘Šåˆ†æå™¨] [handler] è°ƒç”¨å…³è”åˆ†ææ–¹æ³•")
            relationship_result = await relationship_analyzer.analyze_report_relationships(report_data['report_id'])
            print(f"[INFO] [æŠ¥å‘Šåˆ†æå™¨] [handler] å…³è”åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {len(relationship_result.get('relations', []))} ä¸ªå…³è”å…³ç³»")
        except Exception as e:
            print(f"[ERROR] [æŠ¥å‘Šåˆ†æå™¨] [handler] å…³è”åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        return {
            'executed': True,
            'reason': f"æˆåŠŸåˆ†æå¹¶å­˜å‚¨æŠ¥å‘Š: {filename}",
            'actions': ['åˆ†æå®Œæˆ', 'å·²å­˜å‚¨åˆ°æ•°æ®åº“', 'å·²é€šçŸ¥ç”¨æˆ·', 'å…³è”åˆ†æå®Œæˆ'],
            'report_id': report_data['report_id'],
            'importance_score': report_data.get('importance_score')
        }
        
    except Exception as e:
        print(f"[é”™è¯¯] æŠ¥å‘Šåˆ†æå¤±è´¥: {e}")
        await context.notify(
            f"âŒ æŠ¥å‘Šåˆ†æå¤±è´¥: {filename}\né”™è¯¯: {str(e)}",
            {"priority": "high"}
        )
        return {
            'executed': False,
            'reason': f"å¤„ç†å¤±è´¥: {str(e)}"
        }


# ============================================================================
# æ ¸å¿ƒåˆ†æå‡½æ•°
# ============================================================================

async def _analyze_report_with_ai(report_text: str, depth: str = "standard") -> Dict[str, Any]:
    """
    ä½¿ç”¨ Claude AI åˆ†ææŠ¥å‘Š
    
    Args:
        report_text: æŠ¥å‘ŠåŸæ–‡
        depth: åˆ†ææ·±åº¦ (quick/standard/deep)
    
    Returns:
        Dict: åˆ†æç»“æœ JSON
    """
    prompt = _build_analysis_prompt(report_text, depth)
    
    result_text = ""
    try:
        async for message in query(prompt=prompt):
            if isinstance(message, AssistantMessage):
                for block in message.content:
                    if isinstance(block, TextBlock):
                        result_text += block.text
    except Exception as e:
        return {"error": f"AI è°ƒç”¨å¤±è´¥: {str(e)}"}
    
    # è§£æ JSON å“åº”
    analysis_result = _parse_json_response(result_text)
    
    # å¦‚æœ AI æ²¡æœ‰è¿”å›æœ‰æ•ˆçš„ text_summaryï¼Œä½¿ç”¨è§„åˆ™æå–
    if "error" not in analysis_result:
        ai_summary = analysis_result.get("text_summary", {})
        has_valid_summary = (
            ai_summary and isinstance(ai_summary, dict) and
            (ai_summary.get("core_views") or ai_summary.get("key_facts"))
        )
        
        if not has_valid_summary:
            text_summary = _extract_text_summary(report_text)
            analysis_result["text_summary"] = text_summary
    
    return analysis_result


def _transform_to_db_format(
    analysis: Dict[str, Any],
    filename: str,
    file_path: Optional[str] = None,
    custom_report_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    å°† AI åˆ†æç»“æœè½¬æ¢ä¸ºæ•°æ®åº“å­˜å‚¨æ ¼å¼
    
    å¯¹åº”æ•°æ®åº“è¡¨: reports (schema.sql)
    """
    report_info = analysis.get('report_info', {})
    summary = analysis.get('summary', {})
    investment = analysis.get('investment_advice', {})
    metrics = analysis.get('key_metrics', {})
    
    # ç”Ÿæˆ report_id
    if custom_report_id:
        report_id = custom_report_id
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        category = report_info.get('category', 'unknown').replace(' ', '_')
        report_id = f"analysis_{category}_{timestamp}"
    
    # æå–åŸå§‹å†…å®¹ (ä» text_summary é‡å»º)
    text_summary = analysis.get('text_summary', {})
    content_parts = []
    if text_summary.get('core_views'):
        content_parts.append("\n".join(text_summary['core_views']))
    if text_summary.get('key_facts'):
        content_parts.append("\n".join(text_summary['key_facts']))
    # å¦‚æœä»text_summaryæå–çš„å†…å®¹ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨å®Œæ•´çš„åˆ†æJSONä½œä¸ºå†…å®¹
    content = "\n\n".join(content_parts) if content_parts else json.dumps(analysis, ensure_ascii=False, indent=2)
    
    return {
        'report_id': report_id,
        'title': report_info.get('title', filename),
        'report_type': report_info.get('type'),
        'category': report_info.get('category'),
        'date_published': report_info.get('date'),
        'sources': report_info.get('sources', []),
        'content': content,  # ç¡®ä¿contentå­—æ®µå§‹ç»ˆæœ‰å†…å®¹ç”¨äºFTS5æœç´¢
        'summary_one_sentence': summary.get('one_sentence'),
        'sentiment': summary.get('sentiment'),
        'key_drivers': summary.get('key_drivers', []),
        'importance_score': metrics.get('importance_score'),
        'urgency_score': metrics.get('urgency_score'),
        'reliability_score': metrics.get('reliability_score'),
        'action': investment.get('action'),
        'target_allocation': investment.get('target_allocation'),
        'timing': investment.get('timing'),
        'holding_period': investment.get('holding_period'),
        'confidence_level': investment.get('confidence_level'),
        'analysis_json': analysis,  # å®Œæ•´ JSON
        'original_file_path': file_path,
        'file_size': len(content) if content else 0
    }


# ============================================================================
# å·¥å…·å‡½æ•° (ä¾›å†…éƒ¨ä½¿ç”¨)
# ============================================================================


# å‘åå…¼å®¹: ä¿ç•™ç‹¬ç«‹ä½¿ç”¨çš„èƒ½åŠ› (é Listener æ¨¡å¼)
class FinancialReportAnalyzer:
    """é‡‘èæŠ¥å‘Šåˆ†æå™¨ - ç‹¬ç«‹ä½¿ç”¨ç‰ˆæœ¬"""
    
    def __init__(self, db_path: str = "data/finance.db"):
        self.db = DatabaseManager(db_path)
    
    async def analyze_and_store(self, report_text: str, filename: str = "report.txt", depth: str = "standard") -> Dict:
        """
        åˆ†ææŠ¥å‘Šå¹¶å­˜å‚¨åˆ°æ•°æ®åº“
        
        Args:
            report_text: æŠ¥å‘Šæ–‡æœ¬å†…å®¹
            filename: æ–‡ä»¶å
            depth: åˆ†ææ·±åº¦ (quick/standard/deep)
        
        Returns:
            {
                'report_id': str,
                'analysis': Dict,
                'db_id': int
            }
        """
        # åˆ†ææŠ¥å‘Š
        analysis = await _analyze_report_with_ai(report_text, depth)
        
        if "error" in analysis:
            return {'error': analysis['error']}
        
        # è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼
        report_data = _transform_to_db_format(analysis, filename)
        
        print("report_data:", report_data)
        # å­˜å‚¨åˆ°æ•°æ®åº“
        db_id = await self.db.upsert_report(report_data)
        
        return {
            'report_id': report_data['report_id'],
            'analysis': analysis,
            'db_id': db_id
        }
    
    async def search_reports(self, query: str = None, **kwargs) -> List[Dict]:
        """æœç´¢æŠ¥å‘Š"""
        return await self.db.search_reports(query=query, **kwargs)
    
    async def get_report(self, report_id: str) -> Optional[Dict]:
        """è·å–å•ä¸ªæŠ¥å‘Š"""
        return await self.db.get_report(report_id)
    
    def generate_readable_summary(self, analysis: Dict) -> str:
        """ç”Ÿæˆæ˜“è¯»æ‘˜è¦"""
        summary = []
        summary.append("=" * 60)
        summary.append("ğŸ“Š é‡‘èæŠ¥å‘Šåˆ†æç»“æœ")
        summary.append("=" * 60)
        
        if "error" in analysis:
            summary.append(f"âŒ åˆ†æå¤±è´¥: {analysis['error']}")
            return "\n".join(summary)
        
        # åŸºæœ¬ä¿¡æ¯
        info = analysis.get("report_info", {})
        summary.append(f"\nğŸ“‹ æŠ¥å‘Šä¿¡æ¯:")
        summary.append(f"  ç±»å‹: {info.get('type', 'N/A')}")
        summary.append(f"  åˆ†ç±»: {info.get('category', 'N/A')}")
        summary.append(f"  æ—¥æœŸ: {info.get('date', 'N/A')}")
        if info.get('sources'):
            summary.append(f"  æ¥æº: {', '.join(info['sources'][:3])}")
        
        # æ ¸å¿ƒæ‘˜è¦
        summ = analysis.get("summary", {})
        summary.append(f"\nğŸ’¡ {summ.get('one_sentence', 'N/A')}")
        summary.append(f"   æƒ…ç»ª: {summ.get('sentiment', 'N/A')}")
        if summ.get('key_drivers'):
            summary.append(f"   é©±åŠ¨: {', '.join(summ['key_drivers'][:3])}")
        
        # å…³é”®æ•°æ®äº®ç‚¹
        key_data = analysis.get("key_data", {})
        if key_data:
            summary.append(f"\nğŸ“ˆ å…³é”®æ•°æ®:")
            for k, v in list(key_data.items())[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                summary.append(f"  â€¢ {k}: {v}")
        
        # å†å²å¯¹æ¯”
        hist = analysis.get("historical_context", {})
        if hist:
            summary.append(f"\nğŸ“š å†å²å¯¹æ¯”:")
            for k, v in hist.items():
                if v and v != 'N/A':
                    summary.append(f"  â€¢ {k}: {v}")
        
        # æ ¸å¿ƒè§‚ç‚¹
        points = analysis.get("main_points", [])
        if points:
            summary.append(f"\nğŸ¯ æ ¸å¿ƒè§‚ç‚¹:")
            for i, point in enumerate(points, 1):
                summary.append(f"  {i}. {point}")
        
        # æ¨èæ ‡çš„
        targets = analysis.get("investment_targets", {})
        if targets.get("recommended"):
            summary.append(f"\nğŸ¯ æ¨èæ ‡çš„:")
            for target in targets["recommended"][:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                name = target.get('name', 'N/A')
                reason = target.get('reason', '')
                price_action = target.get('price_action', '')
                summary.append(f"  âœ… {name}")
                if reason:
                    summary.append(f"     ç†ç”±: {reason[:100]}..." if len(reason) > 100 else f"     ç†ç”±: {reason}")
                if price_action:
                    summary.append(f"     è¡¨ç°: {price_action}")
        
        if targets.get("cautious"):
            summary.append(f"\nâš ï¸  è°¨æ…æ ‡çš„:")
            for target in targets["cautious"][:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                name = target.get('name', 'N/A')
                reason = target.get('reason', '')
                summary.append(f"  ğŸ”¸ {name}: {reason}")
        
        # æŠ•èµ„å»ºè®®
        advice = analysis.get("investment_advice", {})
        summary.append(f"\nğŸ’¼ æŠ•èµ„å»ºè®®:")
        summary.append(f"  æ“ä½œ: {advice.get('action', 'N/A').upper()}")
        summary.append(f"  é…ç½®: {advice.get('target_allocation', 'N/A')}")
        if advice.get('timing'):
            summary.append(f"  æ—¶æœº: {advice.get('timing')}")
        if advice.get('holding_period'):
            summary.append(f"  æŒæœ‰æœŸ: {advice.get('holding_period')}")
        summary.append(f"  ä¿¡å¿ƒ: {advice.get('confidence_level', 'N/A').upper()}")
        
        # é£é™©æç¤º
        risks = analysis.get("risk_warnings", [])
        if risks:
            summary.append(f"\nâš ï¸  é£é™©æç¤º:")
            for i, risk in enumerate(risks[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ª
                if isinstance(risk, dict):
                    risk_type = risk.get('risk_type', 'é£é™©')
                    desc = risk.get('description', '')
                    impact = risk.get('impact_level', '')
                    impact_icon = "ğŸ”´" if impact == "high" else "ğŸŸ¡" if impact == "medium" else "ğŸŸ¢"
                    summary.append(f"  {i}. {impact_icon} {risk_type}: {desc}")
                else:
                    summary.append(f"  {i}. {risk}")
        
        # å…³é”®æ—¶é—´èŠ‚ç‚¹
        timeline = analysis.get("timeline_events", [])
        if timeline:
            summary.append(f"\nğŸ“… å…³é”®æ—¶é—´èŠ‚ç‚¹:")
            for event in timeline[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                date = event.get('date', '')
                evt = event.get('event', '')
                summary.append(f"  â€¢ {date}: {evt}")
        
        # è¯„åˆ†
        metrics = analysis.get("key_metrics", {})
        summary.append(f"\nâ­ è¯„åˆ†: é‡è¦æ€§{metrics.get('importance_score', 'N/A')}/10 "
                      f"ç´§æ€¥æ€§{metrics.get('urgency_score', 'N/A')}/10 "
                      f"å¯é æ€§{metrics.get('reliability_score', 'N/A')}/10")
        
        # æ–‡æœ¬æ‘˜è¦ (è‡ªåŠ¨æå–)
        text_summary = analysis.get("text_summary", {})
        if text_summary:
            summary.append(f"\nğŸ“ æ–‡æœ¬æ‘˜è¦ (è‡ªåŠ¨æå–):")
            
            core_views = text_summary.get("core_views", [])
            if core_views:
                summary.append(f"\n  æ ¸å¿ƒè§‚ç‚¹ ({len(core_views)}æ¡):")
                for i, view in enumerate(core_views[:5], 1):  # æ˜¾ç¤ºå‰5æ¡
                    summary.append(f"    {i}. {view[:80]}..." if len(view) > 80 else f"    {i}. {view}")
            
            key_facts = text_summary.get("key_facts", [])
            if key_facts:
                summary.append(f"\n  å…³é”®æ•°æ® ({len(key_facts)}æ¡):")
                for i, fact in enumerate(key_facts[:5], 1):  # æ˜¾ç¤ºå‰5æ¡
                    summary.append(f"    â€¢ {fact[:80]}..." if len(fact) > 80 else f"    â€¢ {fact}")
            
            framework = text_summary.get("analysis_framework", [])
            if framework:
                summary.append(f"\n  åˆ†ææ¡†æ¶ ({len(framework)}æ¡):")
                for i, method in enumerate(framework[:3], 1):  # æ˜¾ç¤ºå‰3æ¡
                    summary.append(f"    â—† {method[:80]}..." if len(method) > 80 else f"    â—† {method}")
        
        summary.append("=" * 60)
        return "\n".join(summary)
    
    def validate_analysis_completeness(self, analysis: Dict) -> Dict[str, Any]:
        """éªŒè¯åˆ†æç»“æœçš„å®Œæ•´æ€§å¹¶æä¾›è¯¦ç»†æŠ¥å‘Š"""
        validation_report = {
            "overall_score": 0,
            "completeness_percentage": 0,
            "missing_fields": [],
            "weak_fields": [],
            "strong_fields": [],
            "suggestions": []
        }
        
        # å®šä¹‰å¿…éœ€å­—æ®µå’Œæƒé‡
        required_fields = {
            "report_info": 10,
            "summary": 10,
            "key_data": 15,
            "main_points": 10,
            "investment_advice": 15,
            "key_metrics": 5
        }
        
        # æ¨èå­—æ®µ(åŠ åˆ†é¡¹)
        recommended_fields = {
            "historical_context": 10,
            "investment_targets": 15,
            "risk_warnings": 10,
            "timeline_events": 5,
            "industry_structure": 5,
            "quantitative_metrics": 5,
            "text_summary": 5  # æ–°å¢: æ–‡æœ¬æ‘˜è¦å­—æ®µ
        }
        
        total_score = 0
        max_score = sum(required_fields.values()) + sum(recommended_fields.values())
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field, weight in required_fields.items():
            if field in analysis and analysis[field]:
                # æ£€æŸ¥å­—æ®µå†…å®¹è´¨é‡
                content = analysis[field]
                if isinstance(content, dict):
                    filled_ratio = sum(1 for v in content.values() if v and v != 'N/A') / max(len(content), 1)
                    score = weight * filled_ratio
                elif isinstance(content, list):
                    score = weight if len(content) > 0 else 0
                else:
                    score = weight
                
                total_score += score
                if score >= weight * 0.8:
                    validation_report["strong_fields"].append(field)
                elif score < weight * 0.5:
                    validation_report["weak_fields"].append(field)
                    validation_report["suggestions"].append(f"{field}å­—æ®µä¿¡æ¯ä¸å¤Ÿå®Œæ•´ï¼Œå»ºè®®è¡¥å……æ›´å¤šç»†èŠ‚")
            else:
                validation_report["missing_fields"].append(field)
                validation_report["suggestions"].append(f"ç¼ºå°‘å¿…éœ€å­—æ®µï¼š{field}")
        
        # æ£€æŸ¥æ¨èå­—æ®µ
        for field, weight in recommended_fields.items():
            if field in analysis and analysis[field]:
                content = analysis[field]
                if isinstance(content, dict):
                    filled_ratio = sum(1 for v in content.values() if v and v != 'N/A') / max(len(content), 1)
                    score = weight * filled_ratio
                elif isinstance(content, list):
                    score = weight if len(content) > 0 else weight * 0.5
                else:
                    score = weight
                
                total_score += score
                if score >= weight * 0.8:
                    validation_report["strong_fields"].append(field)
        
        # ç‰¹åˆ«æ£€æŸ¥ï¼šinvestment_targets æ˜¯å¦æœ‰å…·ä½“å…¬å¸ä¿¡æ¯
        if "investment_targets" in analysis:
            targets = analysis["investment_targets"]
            if targets.get("recommended"):
                has_detailed_info = any(
                    t.get("key_metrics") or t.get("price_action") or t.get("market_share")
                    for t in targets["recommended"]
                )
                if not has_detailed_info:
                    validation_report["suggestions"].append(
                        "æ¨èæ ‡çš„ç¼ºå°‘è¯¦ç»†çš„è´¢åŠ¡æ•°æ®ã€è‚¡ä»·è¡¨ç°æˆ–å¸‚åœºä»½é¢ä¿¡æ¯"
                    )
        
        # æ£€æŸ¥ key_data æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹
        if "key_data" in analysis:
            if len(analysis["key_data"]) < 5:
                validation_report["suggestions"].append(
                    f"key_dataä»…æœ‰{len(analysis['key_data'])}ä¸ªæ•°æ®ç‚¹ï¼Œå»ºè®®æå–æ›´å¤šå…³é”®æ•°å­—"
                )
        
        # è®¡ç®—æ€»åˆ†
        validation_report["overall_score"] = round(total_score, 2)
        validation_report["completeness_percentage"] = round((total_score / max_score) * 100, 2)
        
        # æ€»ä½“è¯„ä»·
        if validation_report["completeness_percentage"] >= 80:
            validation_report["grade"] = "ä¼˜ç§€"
            validation_report["summary"] = "åˆ†æéå¸¸å…¨é¢è¯¦ç»†ï¼ŒåŒ…å«äº†ç»å¤§å¤šæ•°å…³é”®ä¿¡æ¯"
        elif validation_report["completeness_percentage"] >= 60:
            validation_report["grade"] = "è‰¯å¥½"
            validation_report["summary"] = "åˆ†æè´¨é‡è¾ƒå¥½ï¼Œä½†è¿˜æœ‰æ”¹è¿›ç©ºé—´"
        else:
            validation_report["grade"] = "éœ€æ”¹è¿›"
            validation_report["summary"] = "åˆ†æä¿¡æ¯ä¸å¤Ÿå®Œæ•´ï¼Œå»ºè®®é‡æ–°åˆ†ææˆ–è¡¥å……æ›´å¤šç»†èŠ‚"
        
        return validation_report


# ============================================================================
# ç‹¬ç«‹ä½¿ç”¨æ¥å£ (å‘½ä»¤è¡Œæ¨¡å¼)
# ============================================================================

def load_report_from_file(file_path: Union[str, Path]) -> Optional[str]:
    """
    ä»æ–‡ä»¶è¯»å–æŠ¥å‘Šå†…å®¹
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
    
    Returns:
        str: æŠ¥å‘Šå†…å®¹,å¤±è´¥è¿”å› None
    """
    path_obj = Path(file_path)
    
    if not path_obj.exists() or not path_obj.is_file():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    
    if path_obj.suffix.lower() not in ['.txt', '.md', '.text']:
        print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path_obj.suffix}")
        return None
    
    try:
        with open(path_obj, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        if content:
            print(f"âœ… å·²åŠ è½½æ–‡ä»¶: {path_obj.name} ({len(content)} å­—ç¬¦)")
            return content
        else:
            print(f"âš ï¸ æ–‡ä»¶ä¸ºç©º: {path_obj.name}")
            return None
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None


# å‘½ä»¤è¡Œä½¿ç”¨å…¥å£ (ç‹¬ç«‹æ¨¡å¼)
async def main_cli(input_file: Optional[str] = None):
    """
    å‘½ä»¤è¡Œæ¨¡å¼ä¸»å‡½æ•°
    
    Usage:
        python report_analyzer.py --input report.txt
    """
    print("ğŸš€ é‡‘èæŠ¥å‘Šæ™ºèƒ½åˆ†æå™¨ (Finance Agent Listener)")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = FinancialReportAnalyzer()
    
    # åŠ è½½æŠ¥å‘Š
    if input_file:
        content = load_report_from_file(input_file)
        filename = Path(input_file).name
    else:
        print("\nğŸ“ ä½¿ç”¨ç¤ºä¾‹æŠ¥å‘Š...\n")
        filename = "ç¤ºä¾‹æŠ¥å‘Š.txt"
        content = """ä¸­å›½å¤®è¡Œç»§ç»­å¢æŒé»„é‡‘ï¼ŒåŠ ä¸Šç¾å›½å…³ç¨æˆ˜å‡ºç°æ–°å˜æ•°ï¼Œå›½é™…é‡‘ä»·ç»´æŒåœ¨3350ç¾å…ƒ/ç›å¸çš„é«˜ä½ã€‚

7æœˆ7æ—¥æ•°æ®æ˜¾ç¤ºï¼Œ6æœˆä»½ç»§ç»­å¢åŠ äº†7ä¸‡ç›å¸çš„é»„é‡‘å‚¨å¤‡ï¼Œè¿™æ˜¯è¿ç»­ç¬¬8ä¸ªæœˆå¢æŒã€‚

æŠ•èµ„å»ºè®®ï¼š
1. ç»§ç»­ä¿æŒä¸€å®šæ¯”ä¾‹çš„é»„é‡‘æŠ•èµ„ï¼Œä¸­é•¿æœŸçœ‹æ¶¨
2. å·²æœ‰æŠ•èµ„è€…ä¸è¦è¿‡å¤šåŠ ä»“ï¼Œå»ºè®®ä¸è¶…è¿‡æ€»èµ„äº§çš„5-10%
3. æ–°æŠ•èµ„è€…å¯åœ¨éœ‡è¡æ—¶é€‚åº¦å‚ä¸ï¼Œé€šè¿‡å®šæŠ•æ–¹å¼é™ä½é£é™©"""
    
    if not content:
        print("âŒ æ— æ³•åŠ è½½æŠ¥å‘Šå†…å®¹")
        return
    
    # åˆ†æå¹¶å­˜å‚¨
    print(f"\nğŸ“– æ­£åœ¨åˆ†æ: {filename}\n")
    result = await analyzer.analyze_and_store(content, filename)
    
    if 'error' in result:
        print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
        return
    
    print(f"âœ… åˆ†æå®Œæˆ!")
    print(f"   æŠ¥å‘ŠID: {result['report_id']}")
    print(f"   æ•°æ®åº“ID: {result['db_id']}")
    print(f"\nğŸ’¾ å·²å­˜å‚¨åˆ°æ•°æ®åº“: data/finance.db")
    
    # æ˜¾ç¤ºæ‘˜è¦
    analysis = result['analysis']
    summary = analysis.get('summary', {})
    print(f"\nğŸ“Š åˆ†ææ‘˜è¦:")
    print(f"   {summary.get('one_sentence', 'N/A')}")
    print(f"   æƒ…ç»ª: {summary.get('sentiment', 'N/A')}")
    print(f"   é‡è¦æ€§: {analysis.get('key_metrics', {}).get('importance_score', 'N/A')}/10")


# ============================================================================
# å‘½ä»¤è¡Œå…¥å£
# ============================================================================

if __name__ == "__main__":
    import asyncio
    import argparse
    
    parser = argparse.ArgumentParser(
        description="é‡‘èæŠ¥å‘Šæ™ºèƒ½åˆ†æå™¨ - Listener æ’ä»¶",
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½œä¸º Listener æ’ä»¶è¿è¡Œ (è‡ªåŠ¨åŠ è½½)
  # ç”± ListenersManager è‡ªåŠ¨è°ƒç”¨ handler() å‡½æ•°
  
  # ç‹¬ç«‹å‘½ä»¤è¡Œæ¨¡å¼
  python report_analyzer.py --input report.txt
  
  # ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬
  python report_analyzer.py
        """
    )
    
    parser.add_argument(
        '--input', '-i',
        type=str,
        help='è¾“å…¥æ–‡ä»¶è·¯å¾„ (.txt, .md, .text)'
    )
    
    args = parser.parse_args()
    
    # è¿è¡Œ CLI æ¨¡å¼
    asyncio.run(main_cli(input_file=args.input))
